rm(list = ls())

df <- read.csv("C:\\Users\\nehas\\Documents\\GT 2023\\MGT 6203\\Project\\bank-full.csv", sep=';')

library(dplyr)
library(tidyverse)
library(scales)
library(caret)
library(ROSE)
library(e1071)
library(corrplot)

#-----------------------------------------------------------------------------------------------------------
#CLEANING AND DATA TRANSFORMATION
#check for missing values
colSums(is.na(df))
#from the output we can see that there are no NULL values for any attribute 

#however there are a lot of unknown values 
df %>%
  summarise_all(list(~sum(. == "unknown")))

#Since over half the data has unknowns for poutcome, we will remove the entire column (not useful in predicting closing probability)
#There is quite a alot of unknows for contact method as well, remove entire column (not using this variable to predict closing probability )
df = subset(df, select = -c(poutcome, contact))
#There are also a few unknowns for job and education
##we will simply remove these rows (288+ 1875)with unknowns
##Note that deleting rows/data for missing data results in data loss and may introduce potential bias
##But in this case we have well over 45,000 observations and removing about 1200 should not bias the data
table(df$y)
##original data has 13.24 success rate 
df <- df[df$job != "unknown" & df$education != "unknown", ]
table(df$y)
##after removal of rows we still have 13.15% success rate 
##so we are nto really biasing the data, difference is only 0.09% so we should be fine to move forward 

#we will also remove duration column because duration of a call is an aspect that cannot be decided before calling customers
df = subset(df, select = -c(duration))

#CORRELATION MATRIX FOR NUMERIC ATTRIBUTES
par(mfrow=c(1,1))
dfcor <- cor(df[,c(1,6,9,11,12,13)], 
             method = "spearman")

corrplot(dfcor,
         method = "color",
         tl.cex = 0.9,
         number.cex = 0.95,
         addCoef.col = "black")
#we can see there is a strong correlation between previous and pdays.
#so we need to remove one
as.data.frame(table(df$previous))
as.data.frame(table(df$pdays))
df = subset(df, select = -c(pdays))
#after looking at a frequency tabel, i removed pdays since previous may hold valuable data in relationship to our problem 

#OUTLIERS FOR RELEVANT NUMERIC ATTRIBUTES 
#unique values 
apply(df,2,function(x) length(unique(x)))
#boxplots
par(mfrow=c(2,2))
boxplot(df$age, df$balance, df$campaign, df$previous)
boxplot(df$age, xlab = "Age")
boxplot(df$balance, xlab = "Balance")
boxplot(df$campaign, xlab = "Campaign")
boxplot(df$previous, xlab = "Previous")
# it seems like there are a lot of outliers for each numeric attribute, especially Balance
# however we cannot say for sure they are outliers without other conext 
summary(df$balance)
par(mfrow=c(1,1))
ggplot(df,aes(x = df$balance)) + geom_histogram(aes(y = after_stat(count / sum(count))), color="black", fill="light blue", binwidth = 500) + 
  labs(title = "Balance Amount Distribution") + xlim(-1000,10000) +
  stat_bin(
    binwidth = 500, geom = "text", color = "black",
    aes(y = after_stat(count / sum(count)), 
        label = scales::percent(after_stat(count / sum(count)))),
    position = position_stack(vjust = 0.9)
  )+
  scale_y_continuous(labels = scales::percent)
#based on this plot lets only include observations with balances from -1000 to 10,000
exclude <- c(-8020:-1001, 10001:102128)

df <- df %>%
  filter(!(df$balance %in% exclude))
summary(df$balance)
as.data.frame(table(df$y))
#yes responses are still 13.08% which is still good 

#CHANGE FROM CHR TO FACTOR 
df$job = as.factor(df$job)
df$marital = as.factor(df$marital)
df$education = as.factor(df$education)
df$default = as.factor(df$default)
df$housing = as.factor(df$housing)
df$loan = as.factor(df$loan)
df$y = as.factor(df$y)

#----------------------------------------------------------------------------------

## FEATURE ENGINEERING - ADDING YEAR & DATE VARIABLE 
#adding addtional variables 
df <- df %>% mutate(sequence = seq_along(month))

# Vector for storing year
years <- rep(2008, nrow(df))

# Iterating through  "month" column - assign year
for (i in 1:(nrow(df) - 1)) {
  month <- df$month[i]
  if (month == "dec" && df$month[i + 1] == "jan") {
    years[(i + 1):nrow(df)] <- years[i] + 1
  }
}

# Adding "year" column - maintaining the order
df <- df %>% mutate(year = years) %>%
  arrange(sequence) %>%
  select(-sequence)

# For 2008-2010 dataset , creating Date using month and day and year we assigned 
df$Date <- as.Date(paste(df$year, df$month, df$day, sep = "-"), format = "%Y-%b-%d")

# TIME SERIES PLOT  
#plot of entire data 
ggplot(df, aes(x =Date)) + 
  geom_line(stat = "count", aes(color = y), size = 1) +
  theme_minimal() + scale_x_date(date_labels = "%b-%Y", date_breaks  ="1 month") +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) + 
  xlab("Month") +  ylab("Count of Clients") + ggtitle("Clients Who Opened a Term Deposit VS Not by Month") 

# some trends: 
# we can see that there is a peak from Nov2008 to Dec2008
# we can also see that there are greater sucesses from Mid April to Mid June 2009
# could be that we saw more activity during the summer months
# through 2008 to 2010, its interesting how the no responses tend to mirror the yes responses
# this could be because there are more no and less yes responses 
#-------------------------------------------------------------------------------------
#CHECKING FOR BIAS/IMBALANCE
# The dataset is already biased in that there are a lot of unscessful responses compared to sucesses
# checking target imbalance 
tgt_imb <- table(df$y)
tgt_imb

#SPLIT DATA AND TREATING IMBALANCE USING ROSE 
# Random seeding so every run gives consistent data set 
# set.seed(123)
# 
# # Splitting the data into train and test sets
# train_indices <- createDataPartition(df$y, p = 0.7, list = FALSE)
# train_data <- df[train_indices, ]
# test_data <- df[-train_indices, ]

set.seed(123)
index<-createDataPartition(df$y,p=0.7,list=FALSE)
train<-df[index,]
test<-df[-index,]

#count of responses for yes and no are imbalanced for train data 
train_tgt_imb <- table(train$y)
print(train_tgt_imb)

test_tgt_imb <- table(test$y)
print(test_tgt_imb)

#undersampled-sample the majority class with ROSE
train_balsam <- ovun.sample(y ~ ., data = train, seed = 123, method = "both")$data
table(train_balsam$y)
#responses are now balanced 

#---------------------------------------------------------------------------------------

#SVM MODEL 

#note: this code for tuning is commented out as it takes hours to run, however results are shown below after based on the tune model below
#tuning the model to find the best parameters for cost and gamma
# start_time <- Sys.time()
# tune_out=tune(svm, y ~ . -year -Date -day -month ,data=train_balsam,
#               type = "C-classification",
#               kernel = "radial",
#               ranges = list( cost = c(0.1,1,10,100) , gamma = c(0.001,0.01,0.1,0.90)))
# end_time <- Sys.time()
# time.elapse <- (end_time - start_time)
# print(time.elapse)
# summary(tune_out)

#RESULTS FOR TUNING
# Time difference of 8.44494 hours
# Parameter tuning of b
